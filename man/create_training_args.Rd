% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/finetuner.R
\name{create_training_args}
\alias{create_training_args}
\title{Create User-Friendly Training Arguments}
\usage{
create_training_args(
  output_dir,
  num_train_epochs = 3L,
  learning_rate = 2e-05,
  per_device_train_batch_size = 16L,
  per_device_eval_batch_size = 32L,
  metric_for_best_model = NULL,
  task_type = "classification",
  seed = 42,
  ...
)
}
\arguments{
\item{output_dir}{Path to the directory where model checkpoints will be saved.}

\item{num_train_epochs}{Number of times to iterate over the training dataset.}

\item{learning_rate}{The initial learning rate for the AdamW optimizer.}

\item{per_device_train_batch_size}{The batch size for training.}

\item{per_device_eval_batch_size}{The batch size for validation.}

\item{metric_for_best_model}{The metric used to identify the best model.
Defaults to "f1" for classification and "mse" for regression.}

\item{task_type}{A string to set a default for `metric_for_best_model`.}

\item{seed}{A random seed for reproducibility for the training process.}

\item{...}{Other arguments to be passed directly to `transformers$TrainingArguments`.}
}
\value{
A `TrainingArguments` object.
}
\description{
A helper function to simplify the creation of `TrainingArguments` that uses
argument names consistent with the Python `transformers` library.
}
