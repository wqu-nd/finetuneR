% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/finetuner.R
\name{create_training_args}
\alias{create_training_args}
\title{Create User-Friendly Training Arguments}
\usage{
create_training_args(
  output_dir,
  num_train_epochs = 10L,
  learning_rate = 2e-05,
  per_device_train_batch_size = 16L,
  per_device_eval_batch_size = 32L,
  warmup_steps = 0L,
  weight_decay = 0.01,
  metric_for_best_model = NULL,
  evaluation_strategy = "epoch",
  logging_strategy = "epoch",
  show_progress_bar = TRUE,
  task_type = "classification",
  seed = 11,
  ...
)
}
\arguments{
\item{output_dir}{Path to the directory where model checkpoints will be saved.}

\item{num_train_epochs}{Number of times to iterate over the training dataset.}

\item{learning_rate}{The initial learning rate for the AdamW optimizer.}

\item{per_device_train_batch_size}{The batch size for training.}

\item{per_device_eval_batch_size}{The batch size for validation.}

\item{warmup_steps}{Number of steps for the learning rate warmup. Defaults to 0.}

\item{weight_decay}{The weight decay to apply (if not zero). Defaults to 0.01.}

\item{metric_for_best_model}{The metric used to identify the best model.
Defaults to "f1" for classification and "mse" for regression.}

\item{evaluation_strategy}{The evaluation and save strategy to adopt during training.
Possible values are `"no"`, `"steps"`, `"epoch"`. Defaults to `"epoch"`.}

\item{logging_strategy}{The logging strategy to adopt during training.
Possible values are `"no"`, `"steps"`, `"epoch"`. Defaults to `"epoch"`.}

\item{show_progress_bar}{A logical value. If `FALSE`, disables the detailed,
per-step progress bar. Defaults to `TRUE`.}

\item{task_type}{A string to set a default for `metric_for_best_model`.}

\item{seed}{A random seed for reproducibility for the training process.}

\item{...}{Other arguments to be passed directly to `transformers$TrainingArguments`.}
}
\value{
A `TrainingArguments` object.
}
\description{
A helper function to simplify the creation of `TrainingArguments`.
}
